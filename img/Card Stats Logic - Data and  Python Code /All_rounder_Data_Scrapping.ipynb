{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a160561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Detailed All-round pagination scrape...\n",
      "  Fetching page 1...\n",
      "  Fetching page 2...\n",
      "  Fetching page 3...\n",
      "  Fetching page 4...\n",
      "  Fetching page 5...\n",
      "  Fetching page 6...\n",
      "  Fetching page 7...\n",
      "  Fetching page 8...\n",
      "  Fetching page 9...\n",
      "  Fetching page 10...\n",
      "  Fetching page 11...\n",
      "  Fetching page 12...\n",
      "  Fetching page 13...\n",
      "  Fetching page 14...\n",
      "  Fetching page 15...\n",
      "  Fetching page 16...\n",
      "  Fetching page 17...\n",
      "  Fetching page 18...\n",
      "  Fetching page 19...\n",
      "  Fetching page 20...\n",
      "  Fetching page 21...\n",
      "  Fetching page 22...\n",
      "  Fetching page 23...\n",
      "  Fetching page 24...\n",
      "  Fetching page 25...\n",
      "  Fetching page 26...\n",
      "  Fetching page 27...\n",
      "  Fetching page 28...\n",
      "  Fetching page 29...\n",
      "  Fetching page 30...\n",
      "  Fetching page 31...\n",
      "  Fetching page 32...\n",
      "  Fetching page 33...\n",
      "  Fetching page 34...\n",
      "  Fetching page 35...\n",
      "  Fetching page 36...\n",
      "  Fetching page 37...\n",
      "  Fetching page 38...\n",
      "  Fetching page 39...\n",
      "  Fetching page 40...\n",
      "  Fetching page 41...\n",
      "  Fetching page 42...\n",
      "  Fetching page 43...\n",
      "  Fetching page 44...\n",
      "  Fetching page 45...\n",
      "  Fetching page 46...\n",
      "  Fetching page 47...\n",
      "  Fetching page 48...\n",
      "  Fetching page 49...\n",
      "  Fetching page 50...\n",
      "  Fetching page 51...\n",
      "  Fetching page 52...\n",
      "  Fetching page 53...\n",
      "  Fetching page 54...\n",
      "  Fetching page 55...\n",
      "  Fetching page 56...\n",
      "  Fetching page 57...\n",
      "  Fetching page 58...\n",
      "  Fetching page 59...\n",
      "  Fetching page 60...\n",
      "  Fetching page 61...\n",
      "  Fetching page 62...\n",
      "  Fetching page 63...\n",
      "  Fetching page 64...\n",
      "  Fetching page 65...\n",
      "\n",
      "Total records scraped before cleaning: 3248\n",
      "Total All-round Records Saved: 3248\n",
      "âœ… All 3248 detailed all-round records saved successfully to AllRounder_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION (All-round) ---\n",
    "BASE_URL = \"https://stats.espncricinfo.com/ci/engine/stats/index.html\"\n",
    "ALLROUND_URL = f\"{BASE_URL}?class=1;orderby=runs;template=results;type=allround\"\n",
    "OUTPUT_FILE = 'AllRounder_Data.csv'\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def extract_headers_robustly(table):\n",
    "    table_head = table.find('thead')\n",
    "    if table_head:\n",
    "        headers = [th.text.strip() for th in table_head.find_all('th')]\n",
    "        table_body = table.find('tbody')\n",
    "        data_rows = table_body.find_all('tr') if table_body else []\n",
    "        if headers:\n",
    "            return headers, data_rows\n",
    "\n",
    "    all_rows = table.find_all('tr')\n",
    "    header_row = None\n",
    "    header_row_index = -1\n",
    "\n",
    "    for i, row in enumerate(all_rows[:5]):\n",
    "        if row.find('th'):\n",
    "            header_row = row\n",
    "            header_row_index = i\n",
    "            break\n",
    "\n",
    "    if header_row:\n",
    "        headers = [cell.text.strip() for cell in header_row.find_all(['th', 'td'])]\n",
    "        data_rows = all_rows[header_row_index + 1:]\n",
    "        return headers, data_rows\n",
    "\n",
    "    return [], []\n",
    "\n",
    "def scrape_single_page(url, headers):\n",
    "    \"\"\"Fetches and parses a single page including Player_ID.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    tables = soup.find_all('table', class_=['engineTable', 'statsTable'])\n",
    "\n",
    "    table = None\n",
    "    for t in tables:\n",
    "        if t.find('th'):\n",
    "            table = t\n",
    "            break\n",
    "\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    headers, rows = extract_headers_robustly(table)\n",
    "    if not headers:\n",
    "        return None\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            row_data = []\n",
    "            for idx, cell in enumerate(cells):\n",
    "                text = cell.text.strip()\n",
    "                if idx == 0:  # Player column\n",
    "                    a_tag = cell.find('a', href=True)\n",
    "                    player_id = None\n",
    "                    if a_tag and a_tag['href']:\n",
    "                        match = re.search(r'/player/(\\d+)\\.html', a_tag['href'])\n",
    "                        if match:\n",
    "                            player_id = match.group(1)\n",
    "                    row_data.append(text)\n",
    "                    row_data.append(player_id)  # Add Player_ID\n",
    "                else:\n",
    "                    row_data.append(text)\n",
    "            data.append(row_data)\n",
    "\n",
    "    headers.insert(1, \"Player_ID\")  # Insert Player_ID in header\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df\n",
    "\n",
    "def scrape_all_pages(base_url, scrape_type):\n",
    "    all_dataframes = []\n",
    "    page = 1\n",
    "    max_pages_reached = False\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "\n",
    "    print(f\"Starting {scrape_type} pagination scrape...\")\n",
    "\n",
    "    while not max_pages_reached:\n",
    "        paged_url = f\"{base_url};page={page}\"\n",
    "        print(f\"  Fetching page {page}...\")\n",
    "        df = scrape_single_page(paged_url, headers)\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            print(f\"  Page {page} returned no data. Ending scrape.\")\n",
    "            max_pages_reached = True\n",
    "        else:\n",
    "            if len(df) < 50:\n",
    "                max_pages_reached = True\n",
    "            all_dataframes.append(df)\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    if all_dataframes:\n",
    "        return pd.concat(all_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# --- CLEANING FUNCTION ---\n",
    "def clean_and_format_allround_data(df):\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.replace('.', '', regex=False)\n",
    "    if '' in df.columns:\n",
    "        df.rename(columns={'': 'Player'}, inplace=True)\n",
    "    if 'Player' not in df.columns and len(df.columns) > 0:\n",
    "        df.rename(columns={df.columns[0]: 'Player'}, inplace=True)\n",
    "\n",
    "    if 'Player' not in df.columns:\n",
    "        print(\"CRITICAL ERROR: 'Player' column could not be identified.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df['Player'] = df['Player'].astype(str).map(str.strip)\n",
    "\n",
    "    col_map = {\n",
    "        'Span': 'Career_Span',\n",
    "        'Mat': 'Matches_Played',\n",
    "        'Runs': 'Total_Runs',\n",
    "        'Bat Av': 'Batting_Average',\n",
    "        '100': 'Centuries',\n",
    "        'Wkts': 'Wickets_Total',\n",
    "        'Bowl Av': 'Bowling_Average',\n",
    "        '5': 'Five_Wickets',\n",
    "        'Ct': 'Catches_Total',\n",
    "        'St': 'Stumpings_Total',\n",
    "        'Ave Diff': 'Average_Difference'\n",
    "    }\n",
    "\n",
    "    df.rename(columns=col_map, errors='ignore', inplace=True)\n",
    "\n",
    "    final_cols = ['Player', 'Player_ID', 'Career_Span', 'Matches_Played', 'Total_Runs',\n",
    "                  'Batting_Average', 'Wickets_Total', 'Bowling_Average', 'Average_Difference',\n",
    "                  'Centuries', 'Five_Wickets', 'Catches_Total', 'Stumpings_Total']\n",
    "\n",
    "    df = df[[c for c in final_cols if c in df.columns]].copy()\n",
    "\n",
    "    numeric_cols = [c for c in df.columns if c not in ['Player', 'Player_ID', 'Career_Span']]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace('-', '0'), errors='coerce').fillna(0)\n",
    "\n",
    "    df_final = df[df['Matches_Played'] > 0].reset_index(drop=True)\n",
    "    print(f\"Total All-round Records Saved: {len(df_final)}\")\n",
    "    return df_final\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    full_allround_df = scrape_all_pages(ALLROUND_URL, \"Detailed All-round\")\n",
    "    if full_allround_df is not None and not full_allround_df.empty:\n",
    "        print(f\"\\nTotal records scraped before cleaning: {len(full_allround_df)}\")\n",
    "        final_df = clean_and_format_allround_data(full_allround_df)\n",
    "        if not final_df.empty:\n",
    "            final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "            print(f\"âœ… All {len(final_df)} detailed all-round records saved successfully to {OUTPUT_FILE}\")\n",
    "        else:\n",
    "            print(\"ðŸ›‘ Cleaning resulted in an empty DataFrame.\")\n",
    "    else:\n",
    "        print(\"ðŸ›‘ Scraping failed or returned no data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
