{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d722af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full Test Batting scrape with pagination...\n",
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Fetching page 21...\n",
      "Fetching page 22...\n",
      "Fetching page 23...\n",
      "Fetching page 24...\n",
      "Fetching page 25...\n",
      "Fetching page 26...\n",
      "Fetching page 27...\n",
      "Fetching page 28...\n",
      "Fetching page 29...\n",
      "Fetching page 30...\n",
      "Fetching page 31...\n",
      "Fetching page 32...\n",
      "Fetching page 33...\n",
      "Fetching page 34...\n",
      "Fetching page 35...\n",
      "Fetching page 36...\n",
      "Fetching page 37...\n",
      "Fetching page 38...\n",
      "Fetching page 39...\n",
      "Fetching page 40...\n",
      "Fetching page 41...\n",
      "Fetching page 42...\n",
      "Fetching page 43...\n",
      "Fetching page 44...\n",
      "Fetching page 45...\n",
      "Fetching page 46...\n",
      "Fetching page 47...\n",
      "Fetching page 48...\n",
      "Fetching page 49...\n",
      "Fetching page 50...\n",
      "Fetching page 51...\n",
      "Fetching page 52...\n",
      "Fetching page 53...\n",
      "Fetching page 54...\n",
      "Fetching page 55...\n",
      "Fetching page 56...\n",
      "Fetching page 57...\n",
      "Fetching page 58...\n",
      "Fetching page 59...\n",
      "Fetching page 60...\n",
      "Fetching page 61...\n",
      "Fetching page 62...\n",
      "Fetching page 63...\n",
      "Fetching page 64...\n",
      "Fetching page 65...\n",
      "Fetching page 66...\n",
      "No more data found. Ending pagination.\n",
      "Scraped total columns: ['Player', 'Span', 'Mat', 'Inns', 'NO', 'Runs', 'HS', 'Ave', '100', '50', '0', '', 'Player_ID']\n",
      "Total Batting Records after cleaning: 3248\n",
      "âœ… All pages data saved successfully with Player IDs to Batting_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_URL = \"https://stats.espncricinfo.com/ci/engine/stats/index.html\"\n",
    "BAT_URL = f\"{BASE_URL}?class=1;orderby=runs;template=results;type=batting\"\n",
    "OUTPUT_FILE = 'Batting_Data.csv'\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def extract_headers_robustly(table):\n",
    "    table_head = table.find('thead')\n",
    "    if table_head:\n",
    "        headers = [th.text.strip() for th in table_head.find_all('th')]\n",
    "        table_body = table.find('tbody')\n",
    "        data_rows = table_body.find_all('tr') if table_body else []\n",
    "        if headers:\n",
    "            return headers, data_rows\n",
    "    all_rows = table.find_all('tr')\n",
    "    header_row = None\n",
    "    header_row_index = -1\n",
    "    for i, row in enumerate(all_rows[:5]):\n",
    "        if row.find('th'):\n",
    "            header_row = row\n",
    "            header_row_index = i\n",
    "            break\n",
    "    if header_row:\n",
    "        headers = [cell.text.strip() for cell in header_row.find_all(['th', 'td'])]\n",
    "        data_rows = all_rows[header_row_index + 1:]\n",
    "        return headers, data_rows\n",
    "    return [], []\n",
    "\n",
    "def scrape_single_page(url):\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    tables = soup.find_all('table', class_=['engineTable', 'statsTable'])\n",
    "    table = None\n",
    "    for t in tables:\n",
    "        if t.find('th'):\n",
    "            table = t\n",
    "            break\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    headers, rows = extract_headers_robustly(table)\n",
    "    if not headers:\n",
    "        return None\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            # Extract Player ID from link\n",
    "            link = row.find('a', href=True)\n",
    "            pid = None\n",
    "            if link:\n",
    "                match = re.search(r'player/(\\d+)', link['href'])\n",
    "                if match:\n",
    "                    pid = match.group(1)\n",
    "            if len(row_data) == len(headers):\n",
    "                row_data.append(pid)\n",
    "                data.append(row_data)\n",
    "\n",
    "    headers.append('Player_ID')\n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "def scrape_all_pages(base_url):\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        paged_url = f\"{base_url};page={page}\"\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        df = scrape_single_page(paged_url)\n",
    "        if df is None or df.empty:\n",
    "            print(\"No more data found. Ending pagination.\")\n",
    "            break\n",
    "        all_data.append(df)\n",
    "        page += 1\n",
    "        time.sleep(1)  # polite delay to not overload server\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def clean_and_format_batting_data(df):\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.replace('.', '', regex=False)\n",
    "    if '' in df.columns:\n",
    "        df.rename(columns={'': 'Player'}, inplace=True)\n",
    "    if 'Player' not in df.columns and len(df.columns) > 0:\n",
    "        df.rename(columns={df.columns[0]: 'Player'}, inplace=True)\n",
    "\n",
    "    df['Player'] = df['Player'].astype(str).map(str.strip)\n",
    "\n",
    "    col_map = {\n",
    "        'Span': 'Career_Span',\n",
    "        'Mat': 'Matches_Played',\n",
    "        'Inns': 'Innings_Batted',\n",
    "        'NO': 'Not_Outs',\n",
    "        'Runs': 'Total_Runs',\n",
    "        'HS': 'Highest_Score',\n",
    "        'Ave': 'Batting_Average',\n",
    "        '100': 'Centuries',\n",
    "        '50': 'Fifties',\n",
    "        '0': 'Ducks_Career',\n",
    "        'Ct': 'Catches',\n",
    "        'St': 'Stumpings'\n",
    "    }\n",
    "\n",
    "    df.rename(columns=col_map, errors='ignore', inplace=True)\n",
    "\n",
    "    final_cols = ['Player', 'Player_ID', 'Career_Span', 'Matches_Played', 'Innings_Batted',\n",
    "                  'Not_Outs', 'Total_Runs', 'Highest_Score', 'Batting_Average',\n",
    "                  'Centuries', 'Fifties', 'Ducks_Career', 'Catches', 'Stumpings']\n",
    "\n",
    "    df = df[[col for col in final_cols if col in df.columns]].copy()\n",
    "\n",
    "    numeric_cols = [c for c in df.columns if c not in ['Player', 'Career_Span', 'Highest_Score', 'Player_ID']]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace('-', '0'), errors='coerce').fillna(0)\n",
    "\n",
    "    if 'Highest_Score' in df.columns:\n",
    "        df['Highest_Score'] = df['Highest_Score'].astype(str).str.replace('*', '', regex=False)\n",
    "        df['Highest_Score'] = pd.to_numeric(df['Highest_Score'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    df_final = df[df['Matches_Played'] > 0].reset_index(drop=True)\n",
    "    print(f\"Total Batting Records after cleaning: {len(df_final)}\")\n",
    "    return df_final\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting full Test Batting scrape with pagination...\")\n",
    "    all_bat_df = scrape_all_pages(BAT_URL)\n",
    "    if not all_bat_df.empty:\n",
    "        print(f\"Scraped total columns: {all_bat_df.columns.tolist()}\")\n",
    "        final_df = clean_and_format_batting_data(all_bat_df)\n",
    "        final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"âœ… All pages data saved successfully with Player IDs to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"ðŸ›‘ No data was scraped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
